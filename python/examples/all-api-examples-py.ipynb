{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of all-api-examples-py.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/ntrajic/nucleus-sdk/blob/master/python/examples/all-api-examples-py.ipynb","timestamp":1556513086186}]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"t1BVTowDALvc","colab_type":"text"},"cell_type":"markdown","source":["Copyright (c) 2018-2019 SumUp Analytics, Inc. All Rights Reserved.\n","\n","NOTICE: All information contained herein is, and remains the property of SumUp Analytics Inc. and its suppliers, if any. The intellectual and technical concepts contained herein are proprietary to SumUp Analytics Inc. and its suppliers and may be covered by U.S. and Foreign Patents, patents in process, and are protected by trade secret or copyright law.\n","\n","Dissemination of this information or reproduction of this material is strictly forbidden unless prior written permission is obtained from SumUp Analytics Inc."]},{"metadata":{"id":"OIc_Z-W9AL1B","colab_type":"text"},"cell_type":"markdown","source":["# Initialization, configure API host and key, and create new API instance"]},{"metadata":{"id":"pheThMgCJ5Tu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":292},"outputId":"7975d40a-9e94-44f2-807e-12693322eb47","executionInfo":{"status":"ok","timestamp":1556509342770,"user_tz":420,"elapsed":4604,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["pip install nucleus_api --upgrade"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting nucleus_api\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/37/1b62eec19b7e0c86aed022514017c798c13a936bae13614dddcb1ce16c02/nucleus_api-2.0.2-py3-none-any.whl (228kB)\n","\u001b[K    100% |████████████████████████████████| 235kB 8.9MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.6/dist-packages (from nucleus_api) (2019.3.9)\n","Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.6/dist-packages (from nucleus_api) (2.5.3)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from nucleus_api) (1.16.3)\n","Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from nucleus_api) (3.0.3)\n","Requirement already satisfied, skipping upgrade: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from nucleus_api) (1.12.0)\n","Requirement already satisfied, skipping upgrade: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from nucleus_api) (1.24.2)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nucleus_api) (0.10.0)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nucleus_api) (1.0.1)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nucleus_api) (2.4.0)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->nucleus_api) (40.9.0)\n","Installing collected packages: nucleus-api\n","Successfully installed nucleus-api-2.0.2\n"],"name":"stdout"}]},{"metadata":{"id":"RVsMWw5GKDva","colab_type":"code","colab":{}},"cell_type":"code","source":["import nucleus_api "],"execution_count":0,"outputs":[]},{"metadata":{"id":"_wTHqHmmAL1F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0f2a7ec0-bb3c-4d83-fba4-84677f8bfd51","executionInfo":{"status":"ok","timestamp":1556509357939,"user_tz":420,"elapsed":330,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["import os\n","import csv\n","import json\n","import datetime\n","import time\n","import nucleus_api\n","from nucleus_api.rest import ApiException\n","import nucleus_api.api.nucleus_api as nucleus_helper\n","from pprint import pprint\n","import numpy as np\n","from pathlib import Path\n","\n","# Determine if in Jupyter notebook or not\n","try:\n","    ip = get_ipython()\n","    running_notebook = True\n","except NameError:\n","    running_notebook = False\n","\n","if running_notebook:\n","    print('Running example in Jupyter Notebook')\n","else:\n","    print('Running example in script mode')\n","    \n","configuration = nucleus_api.Configuration()\n","configuration.host = 'https://7h4tcw9nej.execute-api.us-west-2.amazonaws.com/v2'\n","configuration.api_key['x-api-key'] = '4q3gwVzVjU7Br4lgSg_'\n","\n","# Create API instance\n","api_instance = nucleus_api.NucleusApi(nucleus_api.ApiClient(configuration))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Running example in Jupyter Notebook\n"],"name":"stdout"}]},{"metadata":{"id":"uxidc6esMO1y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"ecafdfc8-bacc-4d11-906a-4c780949c4fa","executionInfo":{"status":"ok","timestamp":1556510292609,"user_tz":420,"elapsed":4536,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["!pwd\n","\n","!ls\n","\n","!git clone https://github.com/SumUpAnalytics/nucleus-sdk.git\n"],"execution_count":27,"outputs":[{"output_type":"stream","text":["/content\n","sample_data\n","ls: cannot access 'data': No such file or directory\n","Cloning into 'nucleus-sdk'...\n","remote: Enumerating objects: 15, done.\u001b[K\n","remote: Counting objects: 100% (15/15), done.\u001b[K\n","remote: Compressing objects: 100% (12/12), done.\u001b[K\n","remote: Total 1008 (delta 7), reused 5 (delta 3), pack-reused 993\u001b[K\n","Receiving objects: 100% (1008/1008), 6.58 MiB | 20.16 MiB/s, done.\n","Resolving deltas: 100% (675/675), done.\n"],"name":"stdout"}]},{"metadata":{"id":"-pB6wWDTNXFu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":459},"outputId":"6ef9b880-effd-41ac-c67e-9f9b0dee0087","executionInfo":{"status":"ok","timestamp":1556511213843,"user_tz":420,"elapsed":2203,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["\n","!ls -alt \n","!ls -alt nucleus-sdk/\n","!ls -alt nucleus-sdk/data/*"],"execution_count":37,"outputs":[{"output_type":"stream","text":["total 20\n","drwxr-xr-x 8 root root 4096 Apr 29 03:59 nucleus-sdk\n","drwxr-xr-x 1 root root 4096 Apr 29 03:59 .\n","drwxr-xr-x 1 root root 4096 Apr 29 03:31 ..\n","drwxr-xr-x 1 root root 4096 Apr  4 20:20 sample_data\n","drwxr-xr-x 1 root root 4096 Apr  4 20:19 .config\n","total 36\n","drwxr-xr-x 8 root root 4096 Apr 29 03:59 .git\n","drwxr-xr-x 8 root root 4096 Apr 29 03:59 .\n","drwxr-xr-x 3 root root 4096 Apr 29 03:59 r\n","drwxr-xr-x 5 root root 4096 Apr 29 03:59 python\n","drwxr-xr-x 4 root root 4096 Apr 29 03:59 javascript\n","drwxr-xr-x 3 root root 4096 Apr 29 03:59 data\n","drwxr-xr-x 2 root root 4096 Apr 29 03:59 docker\n","-rw-r--r-- 1 root root 2290 Apr 29 03:59 README.md\n","drwxr-xr-x 1 root root 4096 Apr 29 03:59 ..\n","-rw-r--r-- 1 root root 51187 Apr 29 03:59 nucleus-sdk/data/quarles20181109a.pdf\n","-rw-r--r-- 1 root root 27958 Apr 29 03:59 nucleus-sdk/data/trump-tweets-100.csv\n","-rw-r--r-- 1 root root   171 Apr 29 03:59 nucleus-sdk/data/custom-sentiment-dict.json\n","\n","nucleus-sdk/data/fomc-minutes:\n","total 856\n","drwxr-xr-x 3 root root   4096 Apr 29 03:59 ..\n","-rw-r--r-- 1 root root 495324 Apr 29 03:59 fomcminutes20181219.pdf\n","drwxr-xr-x 2 root root   4096 Apr 29 03:59 .\n","-rw-r--r-- 1 root root 369090 Apr 29 03:59 fomcminutes20181108.pdf\n"],"name":"stdout"}]},{"metadata":{"id":"QNT9Zhi4AL6q","colab_type":"text"},"cell_type":"markdown","source":["# Dataset APIs"]},{"metadata":{"id":"1MuU7-PGAL6t","colab_type":"text"},"cell_type":"markdown","source":["## Append file from local drive to dataset"]},{"metadata":{"scrolled":true,"id":"cQ-KPOc5AL7D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"99b72f3e-8cd2-4c2b-eb7d-f84297827f5a","executionInfo":{"status":"ok","timestamp":1556511119198,"user_tz":420,"elapsed":603,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["print('--------- Append file from local drive to dataset -----------')\n","dataset = \"dataset_test\"\n","file = 'nucleus-sdk/data/quarles20181109a.pdf'         # file | \n","metadata = {\"time\": \"1/2/2018\", \n","            \"author\": \"Test Author\"}  # Optional json containing additional document metadata\n","\n","try:\n","    api_response = api_instance.post_upload_file(file, dataset, metadata=metadata)\n","    fp = api_response.result\n","    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset,)    \n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","\n","print('-------------------------------------------------------------')"],"execution_count":36,"outputs":[{"output_type":"stream","text":["--------- Append file from local drive to dataset -----------\n","ERROR: Forbidden\n","-------------------------------------------------------------\n"],"name":"stdout"}]},{"metadata":{"id":"_wSbLL_7AL8P","colab_type":"text"},"cell_type":"markdown","source":["# Append all PDFs from a folder to dataset in parallel"]},{"metadata":{"id":"op8CKGqSAL8R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":506},"outputId":"bb7365c5-f706-45fa-f97d-4fcba100b8d7","executionInfo":{"status":"error","timestamp":1556511952785,"user_tz":420,"elapsed":1076,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["folder = 'nucleus-sdk/data/fomc-minutes'         \n","dataset = 'dataset_test'# str | Destination dataset where the file will be inserted.\n","print('--------- Append all files from local folder {} to dataset {} in parallel -----------'.format(folder, dataset))\n","\n","# build file iterable. Each item in the iterable is in the format below:\n","# {'filename': filename,   # filename to be uploaded. REQUIRED\n","#  'metadata': {           # metadata for the file. Optional\n","#      'key1': val1,       # keys can have arbiturary names as long as the names only\n","#      'key2': val2        # contain alphanumeric (0-9|a-z|A-Z) and underscore (_)\n","#   } \n","# }\n","file_iter = []\n","for root, dirs, files in os.walk(folder):\n","    for file in files:\n","        if Path(file).suffix == '.pdf':\n","            file_dict = {'filename': os.path.join(root, file),\n","                         'metadata': {'field1': 'financial'}}\n","            file_iter.append(file_dict)\n","\n","file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=1)\n","for fp in file_props:\n","    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)\n","    \n","print('-------------------------------------------------------------')"],"execution_count":38,"outputs":[{"output_type":"stream","text":["--------- Append all files from local folder nucleus-sdk/data/fomc-minutes to dataset dataset_test in parallel -----------\n","Exception when calling post_upload_file: (403)\n","Reason: Forbidden\n","HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 29 Apr 2019 04:27:20 GMT', 'Content-Type': 'application/json', 'Content-Length': '24', 'Connection': 'keep-alive', 'x-amzn-RequestId': '13fa5859-6a37-11e9-a724-5f2832d10322', 'x-amzn-ErrorType': 'ForbiddenException', 'x-amz-apigw-id': 'Y4h-SGftvHcFQfA='})\n","HTTP response body: {\"message\":\"Forbidden\"}\n","\n","\n","\n","Exception when calling post_upload_file: (403)\n","Reason: Forbidden\n","HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 29 Apr 2019 04:27:20 GMT', 'Content-Type': 'application/json', 'Content-Length': '24', 'Connection': 'keep-alive', 'x-amzn-RequestId': '140cf6a4-6a37-11e9-a019-d1d941e40007', 'x-amzn-ErrorType': 'ForbiddenException', 'x-amz-apigw-id': 'Y4h-TGeQvHcFf4A='})\n","HTTP response body: {\"message\":\"Forbidden\"}\n","\n","\n","\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-300399e8bbb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mfile_props\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnucleus_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_props\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'('\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bytes) has been added to dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------------------------------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'filename'"]}]},{"metadata":{"id":"36LD44-1AL9a","colab_type":"text"},"cell_type":"markdown","source":["## Append file from URL to dataset"]},{"metadata":{"scrolled":true,"id":"dxW6neA_AL9d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"bab8c1ac-9094-4c8f-8ea1-316070ab5f7b","executionInfo":{"status":"ok","timestamp":1556512209687,"user_tz":420,"elapsed":485,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["print('------------ Append file from URL to dataset ---------------')\n","\n","dataset = 'dataset_test'\n","file_url = 'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx'\n","# Optional filename saved on the server for the URL. If not specified, Nucleus will make\n","# an intelligent guess from the file URL\n","filename = 'quarles20181109a-newname.pdf'  \n","payload = nucleus_api.UploadURLModel(\n","                dataset=dataset,\n","                file_url=file_url,\n","                filename=filename  \n","            ) # UploadURLModel | \n","\n","try:\n","    api_response = api_instance.post_upload_url(payload)\n","    url_prop = api_response.result\n","    print(url_prop.file_url, '(', url_prop.size, ' bytes) has been added to dataset', dataset)\n","\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","    \n","print('-------------------------------------------------------------')"],"execution_count":39,"outputs":[{"output_type":"stream","text":["------------ Append file from URL to dataset ---------------\n","ERROR: Forbidden\n","-------------------------------------------------------------\n"],"name":"stdout"}]},{"metadata":{"id":"1fIOzMljAL-w","colab_type":"text"},"cell_type":"markdown","source":["## Append files from URLs to dataset in parallel"]},{"metadata":{"scrolled":true,"id":"6u6RZhMdAL_d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":744},"outputId":"4c4c663a-be25-452c-c3c6-a869c68469be","executionInfo":{"status":"error","timestamp":1556512227772,"user_tz":420,"elapsed":762,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["print('------------ Append file from URL to dataset ---------------')\n","\n","dataset = 'dataset_test'\n","file_urls = [\n","    'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx',\n","    'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109b.docx',\n","    'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109c.docx',\n","    'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109d.docx'\n","]\n","\n","url_props = nucleus_helper.upload_urls(api_instance, dataset, file_urls, processes=1)\n","\n","for up in url_props:\n","    print(up.file_url, '(', up.size, ' bytes) has been added to dataset', dataset)\n","    \n","print('-------------------------------------------------------------')"],"execution_count":40,"outputs":[{"output_type":"stream","text":["------------ Append file from URL to dataset ---------------\n","Exception when calling DatasetsApi->post_upload_url: (403)\n","Reason: Forbidden\n","HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 29 Apr 2019 04:31:55 GMT', 'Content-Type': 'application/json', 'Content-Length': '24', 'Connection': 'keep-alive', 'x-amzn-RequestId': 'b7d3d800-6a37-11e9-aa49-af7d800adc2f', 'x-amzn-ErrorType': 'ForbiddenException', 'x-amz-apigw-id': 'Y4ipPFP_vHcFqdg='})\n","HTTP response body: {\"message\":\"Forbidden\"}\n","\n","\n","\n","Exception when calling DatasetsApi->post_upload_url: (403)\n","Reason: Forbidden\n","HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 29 Apr 2019 04:31:55 GMT', 'Content-Type': 'application/json', 'Content-Length': '24', 'Connection': 'keep-alive', 'x-amzn-RequestId': 'b7dd9bc9-6a37-11e9-bec9-5790170894f0', 'x-amzn-ErrorType': 'ForbiddenException', 'x-amz-apigw-id': 'Y4ipQGP1PHcF7vg='})\n","HTTP response body: {\"message\":\"Forbidden\"}\n","\n","\n","\n","Exception when calling DatasetsApi->post_upload_url: (403)\n","Reason: Forbidden\n","HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 29 Apr 2019 04:31:55 GMT', 'Content-Type': 'application/json', 'Content-Length': '24', 'Connection': 'keep-alive', 'x-amzn-RequestId': 'b7e64e7e-6a37-11e9-955d-177971b81703', 'x-amzn-ErrorType': 'ForbiddenException', 'x-amz-apigw-id': 'Y4ipQFrEvHcFVtA='})\n","HTTP response body: {\"message\":\"Forbidden\"}\n","\n","\n","\n","Exception when calling DatasetsApi->post_upload_url: (403)\n","Reason: Forbidden\n","HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 29 Apr 2019 04:31:55 GMT', 'Content-Type': 'application/json', 'Content-Length': '24', 'Connection': 'keep-alive', 'x-amzn-RequestId': 'b7f01221-6a37-11e9-a546-a99661109683', 'x-amzn-ErrorType': 'ForbiddenException', 'x-amz-apigw-id': 'Y4ipREn2PHcFqSw='})\n","HTTP response body: {\"message\":\"Forbidden\"}\n","\n","\n","\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-84b9e1617c24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murl_props\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'('\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' bytes) has been added to dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------------------------------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'file_url'"]}]},{"metadata":{"id":"EZTGQxR8AMIX","colab_type":"text"},"cell_type":"markdown","source":["## Append JSON to dataset"]},{"metadata":{"id":"jl3BCZqiAMIc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"259190cd-fb57-4d17-d37a-92ddfc45b8b3","executionInfo":{"status":"ok","timestamp":1556512241986,"user_tz":420,"elapsed":585,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'dataset_test'\n","print('----------- Append json from to dataset {}-----------------'.format(dataset))\n","\n","document = {\n","    \"title\": \"This a test json title field\",\n","    \"time\": \"2019-01-01\",\n","    \"content\": \"This is a test json content field\"\n","}\n","\n","payload = nucleus_api.Appendjsonparams(dataset=dataset,\n","                                       document=document)\n","\n","try:\n","    api_response = api_instance.post_append_json_to_dataset(payload)\n","    print(api_response.result)\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])"],"execution_count":41,"outputs":[{"output_type":"stream","text":["2019-04-29 04:32:09,556 WARNING Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(1, '[SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2292)'),)': /v2/datasets/append_json_to_dataset\n"],"name":"stderr"},{"output_type":"stream","text":["----------- Append json from to dataset dataset_test-----------------\n","ERROR: Forbidden\n"],"name":"stdout"}]},{"metadata":{"id":"-NJYCbAOAMIu","colab_type":"text"},"cell_type":"markdown","source":["## Append jsons from csv to dataset in parallel"]},{"metadata":{"scrolled":true,"id":"_vhOgwyzAMIx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":248},"outputId":"bcdedbf4-96c6-42bc-a808-753004f96f7a","executionInfo":{"status":"error","timestamp":1556512251721,"user_tz":420,"elapsed":447,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["# This dataset will be used to test all topics and documents APIs\n","csv_file = 'trump-tweets-100.csv'\n","dataset = 'trump_tweets'\n","print('------- Append json from CSV {} to dataset {}-------------'.format(csv_file, dataset))\n","\n","with open(csv_file, encoding='utf-8-sig') as csvfile:\n","    reader = csv.DictReader(csvfile)\n","    json_props = nucleus_helper.upload_jsons(api_instance, dataset, reader, processes=1)\n","    \n","    total_size = 0\n","    total_jsons = 0\n","    for jp in json_props:\n","        total_size += jp.size\n","        total_jsons += 1\n","        \n","    print(total_jsons, 'JSON records (', total_size, 'bytes) appended to', dataset)\n","\n","print('-------------------------------------------------------------')"],"execution_count":42,"outputs":[{"output_type":"stream","text":["------- Append json from CSV trump-tweets-100.csv to dataset trump_tweets-------------\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-24d72d61d41e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------- Append json from CSV {} to dataset {}-------------'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8-sig'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mjson_props\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnucleus_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_jsons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trump-tweets-100.csv'"]}]},{"metadata":{"id":"9gUBuqbvAMLn","colab_type":"text"},"cell_type":"markdown","source":["## List available datasets"]},{"metadata":{"id":"qwRy0hJ9AMLo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":387},"outputId":"21d99115-e378-4226-e822-1dad5b1ba657","executionInfo":{"status":"error","timestamp":1556512261523,"user_tz":420,"elapsed":284,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["print('---------------- List available datasets ---------------------')\n","try:\n","    api_response = api_instance.get_list_datasets()\n","except ApiException as e:\n","    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)\n","\n","list_datasets = api_response.result\n","\n","print(len(list_datasets), 'datasets in the database:')\n","for ds in list_datasets:\n","    print('    ', ds.name)\n","    \n","print('-------------------------------------------------------------')"],"execution_count":43,"outputs":[{"output_type":"stream","text":["---------------- List available datasets ---------------------\n","Exception when calling DatasetsApi->get_list_datasets: (403)\n","Reason: Forbidden\n","HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 29 Apr 2019 04:32:29 GMT', 'Content-Type': 'application/json', 'Content-Length': '24', 'Connection': 'keep-alive', 'x-amzn-RequestId': 'cc36f535-6a37-11e9-802c-131f9a03ad6d', 'x-amzn-ErrorType': 'ForbiddenException', 'x-amz-apigw-id': 'Y4iulGyzvHcF_8w='})\n","HTTP response body: {\"message\":\"Forbidden\"}\n","\n","\n","\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-19ca3724d535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlist_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'datasets in the database:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'api_response' is not defined"]}]},{"metadata":{"id":"RpT-uWCYAMLz","colab_type":"text"},"cell_type":"markdown","source":["## Get dataset information"]},{"metadata":{"id":"vpHHx6WRAML1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"79a7fc01-c053-436d-d1d0-5ad200bac3bf","executionInfo":{"status":"ok","timestamp":1556512300457,"user_tz":420,"elapsed":255,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'dataset_test' # str | Dataset name.\n","print('--------------- Get dataset information from {}-------------------'.format(dataset))\n","\n","query = '' # str | Fulltext query, using mysql MATCH boolean query format. (optional)\n","metadata_selection = '' # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n","time_period = '' # str | Time period selection (optional)\n","period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n","period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n","\n","try:\n","    payload = nucleus_api.DatasetInfo(dataset=dataset, \n","                                    query=query, \n","                                    metadata_selection=metadata_selection, \n","                                    time_period=time_period)\n","    api_response = api_instance.post_dataset_info(payload)\n","    print('Information about dataset', dataset)\n","    print('    Language:', api_response.result.detected_language)\n","    print('    Number of documents:', api_response.result.num_documents)\n","    print('    Time range:', datetime.datetime.fromtimestamp(float(api_response.result.time_range[0])),\n","             'to', datetime.datetime.fromtimestamp(float(api_response.result.time_range[1])))\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","\n","print('-------------------------------------------------------------')"],"execution_count":44,"outputs":[{"output_type":"stream","text":["--------------- Get dataset information from dataset_test-------------------\n","ERROR: Forbidden\n","-------------------------------------------------------------\n"],"name":"stdout"}]},{"metadata":{"id":"PYNg__dfAML-","colab_type":"text"},"cell_type":"markdown","source":["## Delete document"]},{"metadata":{"scrolled":true,"id":"bRxai3cZAMMA","colab_type":"code","colab":{}},"cell_type":"code","source":["#print('--------------------- Delete document -----------------------')\n","#dataset = 'dataset_test'\n","\n","#docid = '1'\n","#payload = nucleus_api.Deletedocumentmodel(dataset=dataset,\n","#                                          docid=docid) # Deletedocumentmodel | \n","\n","#try:\n","#    api_response = api_instance.post_delete_document(payload)\n","#except ApiException as e:\n","#    print(\"Exception when calling DatasetsApi->post_delete_document: %s\\n\" % e)\n","\n","\n","#print('Document', docid, 'from dataset', dataset, 'has been deleted.')\n","## print(api_response)     # raw API response\n","#print('-------------------------------------------------------------')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aHr3fS3SAMMH","colab_type":"text"},"cell_type":"markdown","source":["## Delete dataset"]},{"metadata":{"id":"tbqKF5d_AMMU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"beb7d4a4-aea5-4250-ce45-68cb3375e80b","executionInfo":{"status":"ok","timestamp":1556512316213,"user_tz":420,"elapsed":402,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["print('--------------------- Delete dataset ------------------------')\n","\n","dataset = 'dataset_test'\n","payload = nucleus_api.Deletedatasetmodel(dataset=dataset) # Deletedatasetmodel | \n","\n","try:\n","    api_response = api_instance.post_delete_dataset(payload)\n","    print(api_response.result['result'])\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","    \n","# List datasets again to check if the specified dataset has been deleted\n","try:\n","    api_response = api_instance.get_list_datasets()\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","    \n","print('-------------------------------------------------------------')"],"execution_count":46,"outputs":[{"output_type":"stream","text":["--------------------- Delete dataset ------------------------\n","ERROR: Forbidden\n","ERROR: Forbidden\n","-------------------------------------------------------------\n"],"name":"stdout"}]},{"metadata":{"id":"aq3YHQXnAMMt","colab_type":"text"},"cell_type":"markdown","source":["# Topic APIs"]},{"metadata":{"id":"pzVCpHy-AMMv","colab_type":"text"},"cell_type":"markdown","source":["## Get list of topics from dataset"]},{"metadata":{"scrolled":true,"id":"hY08RXw6AMMz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"a5d6e1e4-3344-431c-fd58-bfc108986f52","executionInfo":{"status":"error","timestamp":1556512340205,"user_tz":420,"elapsed":260,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets'\n","print('------------- Get list of topics from dataset {}--------------'.format(dataset))\n","\n","#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n","query = ''\n","custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n","num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n","metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n","time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n","\n","try:\n","    payload = nucleus_api.Topics(dataset=dataset,                                \n","                                query=query,                   \n","                                custom_stop_words=custom_stop_words,     \n","                                num_topics=num_topics,\n","                                metadata_selection=metadata_selection,\n","                                time_period=time_period)\n","    api_response = api_instance.post_topic_api(payload)        \n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","\n","doc_ids = api_response.result.doc_ids\n","topics = api_response.result.topics\n","for i, res in enumerate(topics):\n","    print('Topic', i, 'keywords:')\n","    print('    Keywords:', res.keywords)\n","    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n","    print('    Keyword weights:', keywords_weight_str)\n","    print('    Strength:', res.strength)\n","    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposures\n","    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n","    for j in range(len(res.doc_topic_exposures)):\n","        doc_topic_exp = float(res.doc_topic_exposures[j])\n","        if doc_topic_exp != 0:\n","            doc_topic_exposure_sel.append(doc_topic_exp)\n","            doc_id_sel.append(doc_ids[j])\n","    \n","    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n","    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n","    print('    Document IDs:', doc_id_sel_str)\n","    print('    Document exposures:', doc_topic_exposure_sel_str)\n","\n","    print('---------------')\n","    \n","print('-------------------------------------------------------------')"],"execution_count":49,"outputs":[{"output_type":"stream","text":["------------- Get list of topics from dataset trump_tweets--------------\n","ERROR: Forbidden\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-0a16f1a686fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdoc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'api_response' is not defined"]}]},{"metadata":{"id":"hfVJCRQHAMM8","colab_type":"text"},"cell_type":"markdown","source":["## Get list of topics from dataset with a time range selection"]},{"metadata":{"scrolled":true,"id":"BDuNu5OcAMM-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"683ccc4c-546e-4c84-8612-ed328c5bb46f","executionInfo":{"status":"error","timestamp":1556512397702,"user_tz":420,"elapsed":434,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets'\n","print('------------- Get list of topics from dataset {}--------------'.format(dataset))\n","\n","#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n","query = ''\n","custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n","num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n","metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n","period_start = \"2016-10-15\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD\"\n","period_end = \"2019-01-01\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD\"\n","\n","try:\n","    payload = nucleus_api.Topics(dataset=dataset,                                \n","                                query=query,                   \n","                                custom_stop_words=custom_stop_words,     \n","                                num_topics=num_topics,\n","                                metadata_selection=metadata_selection,\n","                                period_start=period_start,\n","                                period_end=period_end)\n","    api_response = api_instance.post_topic_api(payload)        \n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","    \n","doc_ids = api_response.result.doc_ids\n","topics = api_response.result.topics\n","for i, res in enumerate(topics):\n","    print('Topic', i, 'keywords:')\n","    print('    Keywords:', res.keywords)\n","    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n","    print('    Keyword weights:', keywords_weight_str)\n","    print('    Strength:', res.strength)\n","    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n","    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n","    for j in range(len(res.doc_topic_exposures)):\n","        doc_topic_exp = float(res.doc_topic_exposures[j])\n","        if doc_topic_exp != 0:\n","            doc_topic_exposure_sel.append(doc_topic_exp)\n","            doc_id_sel.append(doc_ids[j])\n","    \n","    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n","    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n","    print('    Document IDs:', doc_id_sel_str)\n","    print('    Document exposures:', doc_topic_exposure_sel_str)\n","\n","    print('---------------')\n","\n","print('-------------------------------------------------------------')"],"execution_count":51,"outputs":[{"output_type":"stream","text":["------------- Get list of topics from dataset trump_tweets--------------\n","ERROR: Forbidden\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-475d8c28048d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdoc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'api_response' is not defined"]}]},{"metadata":{"id":"JqQDZDhKAMNE","colab_type":"text"},"cell_type":"markdown","source":["## Get list of topics from dataset with a metadata selection"]},{"metadata":{"id":"NBKoHjAGAMNG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"48db3532-0291-4cb1-ff50-51f13f0f4c84","executionInfo":{"status":"error","timestamp":1556512407153,"user_tz":420,"elapsed":284,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets'\n","print('------------- Get list of topics from dataset {}--------------'.format(dataset))\n","\n","#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n","query = ''\n","custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n","num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n","metadata_selection = {\"author\": \"D_Trump16\"} # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n","\n","try:\n","    payload = nucleus_api.Topics(dataset=dataset,                                \n","                                query=query,                   \n","                                custom_stop_words=custom_stop_words,     \n","                                num_topics=num_topics,\n","                                metadata_selection=metadata_selection)\n","    api_response = api_instance.post_topic_api(payload)        \n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","    \n","doc_ids = api_response.result.doc_ids\n","topics = api_response.result.topics\n","for i, res in enumerate(topics):\n","    print('Topic', i, 'keywords:')\n","    print('    Keywords:', res.keywords)\n","    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n","    print('    Keyword weights:', keywords_weight_str)\n","    print('    Strength:', res.strength)\n","    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n","    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n","    for j in range(len(res.doc_topic_exposures)):\n","        doc_topic_exp = float(res.doc_topic_exposures[j])\n","        if doc_topic_exp != 0:\n","            doc_topic_exposure_sel.append(doc_topic_exp)\n","            doc_id_sel.append(doc_ids[j])\n","    \n","    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n","    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n","    print('    Document IDs:', doc_id_sel_str)\n","    print('    Document exposures:', doc_topic_exposure_sel_str)\n","\n","    print('---------------')\n","    \n","print('-------------------------------------------------------------')"],"execution_count":52,"outputs":[{"output_type":"stream","text":["------------- Get list of topics from dataset trump_tweets--------------\n","ERROR: Forbidden\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-0fc580a6b392>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdoc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'api_response' is not defined"]}]},{"metadata":{"id":"i9fgIel2AMOC","colab_type":"text"},"cell_type":"markdown","source":["## Get topic summary"]},{"metadata":{"id":"rgmRpH0DAMOP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"2cb0aeb3-64e4-4220-ddfa-c5da5b5c5bec","executionInfo":{"status":"ok","timestamp":1556512415486,"user_tz":420,"elapsed":421,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets'\n","print('------------------- Get topic summary for {} -----------------------'.format(dataset))\n"," # str | Dataset name.\n","#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n","query = ''\n","custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n","num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n","num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n","summary_length = 6 # int | The maximum number of bullet points a user wants to see in each topic summary. (optional) (default to 6)\n","context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n","num_docs = 20 # int | The maximum number of key documents to use for summarization. (optional) (default to 20)\n","excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n","\n","metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n","time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"]  (optional)\n","period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n","period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n","api_response = None\n","\n","try:\n","    payload = nucleus_api.TopicSummaryModel\t(\n","        dataset=dataset, \n","        query=query,\n","        custom_stop_words=custom_stop_words, \n","        num_topics=num_topics, \n","        num_keywords=num_keywords,\n","        metadata_selection=metadata_selection,\n","        summary_length=summary_length, \n","        context_amount=context_amount, \n","        num_docs=num_docs)\n","    api_response = api_instance.post_topic_summary_api(payload)\n","    api_ok = True\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","    api_ok = False\n","\n","if api_ok:\n","    for i,res in enumerate(api_response.result):\n","        print('Topic', i, 'summary:')\n","        print('    Keywords:', res.keywords)\n","        for j in range(len(res.summary)):\n","            print(res.summary[j])\n","            print('    Document ID:', res.summary[j].sourceid)\n","            print('        Title:', res.summary[j].title)\n","            print('        Sentences:', res.summary[j].sentences)\n","            print('        Author:', res.summary[j].attribute['author'])\n","            print('        Time:', datetime.datetime.fromtimestamp(float(res.summary[j].attribute['time'])))\n","\n","        print('---------------')\n","    \n","print('-------------------------------------------------------------')"],"execution_count":53,"outputs":[{"output_type":"stream","text":["------------------- Get topic summary for trump_tweets -----------------------\n","ERROR: Forbidden\n","-------------------------------------------------------------\n"],"name":"stdout"}]},{"metadata":{"id":"MXywWVICAMOf","colab_type":"text"},"cell_type":"markdown","source":["## Get topic sentiment"]},{"metadata":{"scrolled":true,"id":"IpAnDwezAMOg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"cefaafd9-abd9-404a-e4e1-776d3d24ed87","executionInfo":{"status":"error","timestamp":1556512422749,"user_tz":420,"elapsed":571,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets' # str | Dataset name\n","print('---------------- Get topic sentiment for {} ------------------------'.format(dataset))\n","\n","#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n","query = ''\n","custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n","num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n","num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n","excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n","custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n","\n","metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n","time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n","period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n","period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n","\n","try:\n","    payload = nucleus_api.TopicSentimentModel(\n","        dataset=dataset, \n","        query=query, \n","        custom_stop_words=custom_stop_words, \n","        num_topics=num_topics, \n","        num_keywords=num_keywords,\n","        custom_dict_file=custom_dict_file)\n","    api_response = api_instance.post_topic_sentiment_api(payload)\n","    \n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","\n","for i,res in enumerate(api_response.result):\n","    print('Topic', i, 'sentiment:')\n","    print('    Keywords:', res.keywords)\n","    print('    Sentiment:', res.sentiment)\n","    print('    Strength:', res.strength)\n","    \n","    doc_id_str = ' '.join(str(x) for x in res.doc_ids)\n","    doc_sentiment_str = ' '.join(str(x) for x in res.doc_sentiments)\n","    doc_score_str = ' '.join(str(x) for x in res.doc_topic_exposures)\n","    print('    Document IDs:', doc_id_str)\n","    print('    Document Sentiments:', doc_sentiment_str)\n","    print('    Document Exposures:', doc_score_str)\n","    \n","    print('---------------')\n","    \n","print('-------------------------------------------------------------')"],"execution_count":54,"outputs":[{"output_type":"stream","text":["---------------- Get topic sentiment for trump_tweets ------------------------\n","ERROR: Forbidden\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-54-77a5cbd59846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Topic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentiment:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    Keywords:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'result'"]}]},{"metadata":{"id":"GwXiPMCGAMOo","colab_type":"text"},"cell_type":"markdown","source":["## Get topic consensus"]},{"metadata":{"scrolled":true,"id":"mvq3EzfRAMOq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"d88c0024-19ab-4831-ab56-bfc6fcfec9bd","executionInfo":{"status":"error","timestamp":1556512439723,"user_tz":420,"elapsed":250,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets' # str | Dataset name.\n","print('---------------- Get topic consensus for {} ------------------------'.format(dataset))\n","\n","query = '' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n","custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n","num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n","num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n","excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n","custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n","\n","metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n","time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n","period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n","period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n","\n","try:\n","    payload = nucleus_api.TopicConsensusModel(\n","        dataset=dataset, \n","        query=query, \n","        custom_stop_words=custom_stop_words, \n","        num_topics=num_topics, \n","        num_keywords=num_keywords,\n","        custom_dict_file=custom_dict_file)\n","    api_response = api_instance.post_topic_consensus_api(payload)\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","    \n","for i, res in enumerate(api_response.result):\n","    print('Topic', i, 'consensus:')\n","    print('    Keywords:', res.keywords)\n","    print('    Consensus:', res.consensus)\n","    print('    Strength:', res.strength)\n","    \n","    print('---------------')\n","    \n","print('-------------------------------------------------------------')"],"execution_count":55,"outputs":[{"output_type":"stream","text":["---------------- Get topic consensus for trump_tweets ------------------------\n","ERROR: Forbidden\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-55-bd26f5804c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Topic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'consensus:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    Keywords:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'result'"]}]},{"metadata":{"id":"BQJ61FhlAMO2","colab_type":"text"},"cell_type":"markdown","source":["## Get topic historical analysis"]},{"metadata":{"id":"K93uFn9wAMO3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":421},"outputId":"51c4c5a0-3672-4410-b626-ab195dbb90a3","executionInfo":{"status":"error","timestamp":1556512459508,"user_tz":420,"elapsed":284,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets'   # str | Dataset name.\n","print('------------ Get topic historical analysis for {} ----------------'.format(dataset))\n","\n","update_period = 'm' # str | Frequency at which the historical anlaysis is performed. choices=[\"d\",\"m\",\"H\",\"M\"] (default to d)\n","query = '' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n","custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words (optional)\n","num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n","num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n","inc_step = 1 # int | Number of increments of the udpate period in between two historical computations. (optional) (default to 1)\n","excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n","custom_dict_file = {} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n","\n","metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n","time_period = \"12M\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n","period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n","period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n","api_response = None\n","try:\n","    payload = nucleus_api.TopicHistoryModel(\n","        dataset=dataset, \n","        time_period=time_period, \n","        update_period=update_period, \n","        query=query, \n","        custom_stop_words=custom_stop_words, \n","        num_topics=num_topics, \n","        num_keywords=num_keywords, \n","        metadata_selection=metadata_selection, \n","        inc_step=inc_step, \n","        excluded_docs=excluded_docs,\n","        custom_dict_file=custom_dict_file)\n","    api_response = api_instance.post_topic_historical_analysis_api(payload)\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","    print(e)\n","\n","print('Printing historical metrics data...')\n","print('NOTE: historical metrics data can be plotted when running the example in Jupyter Notebook')\n","\n","for i,res in enumerate(api_response.result):\n","    print('Topic', i, res.keywords)\n","    print('    Timestamps:', res.time_stamps)\n","    print('    Strengths:', res.strengths)\n","    print('    Consensuses:', res.consensuses)\n","    print('    Sentiments:', res.sentiments)\n","    print('----------------')\n","            \n","\n","# chart the historical metrics when running in Jupyter Notebook\n","if running_notebook:\n","    print('Plotting historical metrics data...')\n","    historical_metrics = []\n","    for res in api_response.result:\n","        # construct a list of historical metrics dictionaries for charting\n","        historical_metrics.append({\n","            'topic'    : res.keywords,\n","            'time_stamps' : np.array(res.time_stamps),\n","            'strength' : np.array(res.strengths, dtype=np.float32),\n","            'consensus': np.array(res.consensuses, dtype=np.float32), \n","            'sentiment': np.array(res.sentiments, dtype=np.float32)})\n","\n","    selected_topics = range(len(historical_metrics)) \n","    #nucleus_helper.topic_charts_historical(historical_metrics, selected_topics, True)\n","\n","print('-------------------------------------------------------------')"],"execution_count":56,"outputs":[{"output_type":"stream","text":["------------ Get topic historical analysis for trump_tweets ----------------\n","ERROR: Forbidden\n","(403)\n","Reason: Forbidden\n","HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 29 Apr 2019 04:35:47 GMT', 'Content-Type': 'application/json', 'Content-Length': '24', 'Connection': 'keep-alive', 'x-amzn-RequestId': '423bd105-6a38-11e9-90bc-1154eca86d49', 'x-amzn-ErrorType': 'ForbiddenException', 'x-amz-apigw-id': 'Y4jNhFr-vHcFTFQ='})\n","HTTP response body: {\"message\":\"Forbidden\"}\n","\n","\n","Printing historical metrics data...\n","NOTE: historical metrics data can be plotted when running the example in Jupyter Notebook\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-d2c93e5bd837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NOTE: historical metrics data can be plotted when running the example in Jupyter Notebook'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Topic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    Timestamps:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_stamps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'result'"]}]},{"metadata":{"id":"bruOgiLaAMO_","colab_type":"text"},"cell_type":"markdown","source":["## Get author connectivity"]},{"metadata":{"id":"v2XrpMjDAMPC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"0fb2b179-41ca-4ec4-8c91-38d5820ebba2","executionInfo":{"status":"error","timestamp":1556512478476,"user_tz":420,"elapsed":300,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets' # str | Dataset name.\n","print('----------------- Get author connectivity for {} -------------------'.format(dataset))\n","\n","target_author = 'D_Trump16' # str | Name of the author to be analyzed.\n","query = '' # str | Fulltext query, using mysql MATCH boolean query format. Subject covered by the author, on which to focus the analysis of connectivity. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n","custom_stop_words = [\"real\",\"hillary\"] # str | List of words possibly used by the target author that are considered not information-bearing. (optional)\n","excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n","\n","metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n","time_period = \"12M\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n","period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n","period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n","\n","try:\n","    payload = nucleus_api.AuthorConnection(dataset=dataset, \n","                                            target_author=target_author, \n","                                            query=query, \n","                                            custom_stop_words=custom_stop_words, \n","                                            time_period=time_period, \n","                                            metadata_selection=metadata_selection, \n","                                            excluded_docs=excluded_docs)\n","    api_response = api_instance.post_author_connectivity_api(payload)    \n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","\n","res = api_response.result\n","print('Mainstream connections:')\n","for mc in res.mainstream_connections:\n","    print('    Keywords:', mc.keywords)\n","    print('    Authors:', \" \".join(str(x) for x in mc.authors))\n","    \n","print('Niche connections:')\n","for nc in res.niche_connections:\n","    print('    Keywords:', nc.keywords)\n","    print('    Authors:', \" \".join(str(x) for x in nc.authors))  \n","    \n","print('-------------------------------------------------------------')"],"execution_count":57,"outputs":[{"output_type":"stream","text":["----------------- Get author connectivity for trump_tweets -------------------\n","ERROR: Forbidden\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-57-a4da48a15265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mainstream connections:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainstream_connections\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'result'"]}]},{"metadata":{"id":"9sRggPH5AMPP","colab_type":"text"},"cell_type":"markdown","source":["## Get topic transfer"]},{"metadata":{"id":"ml-p52pLAMPS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":207},"outputId":"50a40c33-05ef-45f5-aad0-162997e22ef0","executionInfo":{"status":"ok","timestamp":1556512490714,"user_tz":420,"elapsed":270,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset0 = 'trump_tweets'\n","print('--------------- Topic transfer from {} ------------------'.format(dataset0))\n","\n","dataset1 = None # str | Validation dataset (optional if period_0 and period_1 dates provided)\n","#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n","query = ''\n","custom_stop_words = [\"\"] # str | List of stop words. (optional)\n","num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n","num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n","metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n","period_0_start = '2018-08-12' # Not needed if you provide a validation dataset in the \"dataset1\" variable \n","period_0_end = '2018-08-16' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n","period_1_start = '2018-08-14' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n","period_1_end = '2018-08-18' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n","excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n","\n","try:\n","    payload = nucleus_api.TopicTransferModel(dataset0=dataset0,\n","                                             dataset1=dataset1,\n","                                             query=query, \n","                                             custom_stop_words=custom_stop_words, \n","                                             num_topics=num_topics, \n","                                             num_keywords=num_keywords,\n","                                             period_0_start=period_0_start,\n","                                             period_0_end=period_0_end,\n","                                             period_1_start=period_1_start,\n","                                             period_1_end=period_1_end,\n","                                             metadata_selection=metadata_selection)\n","    api_response = api_instance.post_topic_transfer_api(payload)\n","    api_ok = True\n","except ApiException as e:\n","    print(e)\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","    api_ok = False\n","\n","print(api_response)\n","\n","if api_ok:\n","    doc_ids_t1 = api_response.result.doc_ids_t1\n","    topics = api_response.result.topics\n","    for i,res in enumerate(topics):\n","        print('Topic', i, 'exposure within validation dataset:')\n","        print('    Keywords:', res.keywords)\n","        print('    Strength:', res.strength)\n","        print('    Document IDs:', doc_ids_t1)\n","        print('    Exposure per Doc in Validation Dataset:', res.doc_topic_exposures_t1)\n","        print('---------------')\n","    \n","print('-------------------------------------------------------------')"],"execution_count":58,"outputs":[{"output_type":"stream","text":["--------------- Topic transfer from trump_tweets ------------------\n","(403)\n","Reason: Forbidden\n","HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 29 Apr 2019 04:36:18 GMT', 'Content-Type': 'application/json', 'Content-Length': '24', 'Connection': 'keep-alive', 'x-amzn-RequestId': '54d86061-6a38-11e9-9a13-4df3ad3142a8', 'x-amzn-ErrorType': 'ForbiddenException', 'x-amz-apigw-id': 'Y4jSZGfsvHcF12w='})\n","HTTP response body: {\"message\":\"Forbidden\"}\n","\n","\n","ERROR: Forbidden\n","None\n","-------------------------------------------------------------\n"],"name":"stdout"}]},{"metadata":{"id":"U8gId6oZAMPa","colab_type":"text"},"cell_type":"markdown","source":["## Get topic transfer when topics are exogenously imposed"]},{"metadata":{"id":"Vus3c3ftAMPc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"0d49d789-d87f-461b-d124-9628d5572652","executionInfo":{"status":"ok","timestamp":1556512503262,"user_tz":420,"elapsed":270,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset0 = 'trump_tweets'\n","print('--------------- Get topic transfer from {} -------------------'.format(dataset0))\n","\n","dataset1 = None # str | Validation dataset (optional if period_0 and period_1 dates provided)\n","fixed_topics = [{\"keywords\": [\"north korea\", \"nuclear weapons\", \"real estate\"], \"weights\": [0.5, 0.3, 0.2]},\n","               {\"keywords\": [\"America\", \"jobs\", \"stock market\"], \"weights\": [0.3, 0.3, 0.3]}] # The weights are optional\n","query = ''\n","custom_stop_words = [\"\"] # str | List of stop words. (optional)\n","num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n","num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n","metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n","period_0_start = '2017-01-01' # Not needed if you provide a validation dataset in the \"dataset1\" variable \n","period_0_end = '2017-12-31' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n","period_1_start = '2018-01-01' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n","period_1_end = '2018-08-18' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n","period_0_start = '2018-08-12' # Not needed if you provide a validation dataset in the \"dataset1\" variable \n","period_0_end = '2018-08-16' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n","period_1_start = '2018-08-14' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n","period_1_end = '2018-08-18' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n","\n","excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n","\n","try:\n","    payload = nucleus_api.TopicTransferModel(dataset0=dataset0,\n","                                             dataset1=dataset1,\n","                                             fixed_topics=fixed_topics,\n","                                             query=query, \n","                                             custom_stop_words=custom_stop_words, \n","                                             num_topics=num_topics, \n","                                             num_keywords=num_keywords,\n","                                             period_0_start=period_0_start,\n","                                             period_0_end=period_0_end,\n","                                             period_1_start=period_1_start,\n","                                             period_1_end=period_1_end,\n","                                             metadata_selection=metadata_selection)\n","    api_response = api_instance.post_topic_transfer_api(payload)\n","    api_ok = True\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","    api_ok = False\n","\n","if api_ok:\n","    doc_ids_t1 = api_response.result.doc_ids_t1\n","    topics = api_response.result.topics\n","    for i,res in enumerate(topics):\n","        print('Topic', i, 'exposure within validation dataset:')\n","        print('    Keywords:', res.keywords)\n","        print('    Strength:', res.strength)\n","        print('    Document IDs:', doc_ids_t1)\n","        print('    Exposure per Doc in Validation Dataset:', res.doc_topic_exposures_t1)\n","        print('---------------')\n","    \n","print('-------------------------------------------------------------')"],"execution_count":59,"outputs":[{"output_type":"stream","text":["--------------- Get topic transfer from trump_tweets -------------------\n","ERROR: Forbidden\n","-------------------------------------------------------------\n"],"name":"stdout"}]},{"metadata":{"id":"fqbhEsCfAMPl","colab_type":"text"},"cell_type":"markdown","source":["## Get topic sentiment transfer"]},{"metadata":{"id":"c9loi7vKAMPn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"08849dad-b891-439d-d120-0017186d67d3","executionInfo":{"status":"ok","timestamp":1556512516647,"user_tz":420,"elapsed":328,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset0 = 'trump_tweets'\n","dataset1 = None\n","print('------------------- Get topic sentiment transfer for {} -----------------------'.format(dataset))\n","\n","#dataset1 = dataset # str | Validation dataset (optional if period_0 and period_1 dates provided)\n","#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n","#fixed_topic is also an available input argument\n","query = ''\n","custom_stop_words = [\"\"] # str | List of stop words. (optional)\n","num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n","num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n","metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n","period_0_start = '2018-08-12' # Not needed if you provide a validation dataset in the \"dataset1\" variable \n","period_0_end = '2018-08-16' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n","period_1_start = '2018-08-14' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n","period_1_end = '2018-08-18' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n","excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n","custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n","\n","try:\n","    payload = nucleus_api.TopicSentimentTransferModel(\n","        dataset0=dataset0, \n","        dataset1=dataset1,\n","        query=query, \n","        custom_stop_words=custom_stop_words, \n","        num_topics=num_topics, \n","        num_keywords=num_keywords,\n","        period_0_start=period_0_start,\n","        period_0_end=period_0_end,\n","        period_1_start=period_1_start,\n","        period_1_end=period_1_end,\n","        metadata_selection=metadata_selection,\n","        custom_dict_file=custom_dict_file)\n","    api_response = api_instance.post_topic_sentiment_transfer_api(payload)\n","    api_ok = True\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","    api_ok = False\n","\n","if api_ok:\n","    topics = api_response.result\n","    for i,res in enumerate(topics):\n","        print('Topic', i, 'exposure within validation dataset:')\n","        print('    Keywords:', res.keywords)\n","        print('    Strength:', res.strength)\n","        print('    Sentiment:', res.sentiment)\n","        print('    Document IDs:', res.doc_ids_t1)\n","        print('    Sentiment per Doc in Validation Dataset:', res.doc_sentiments_t1)\n","        print('---------------')\n","    \n","print('-------------------------------------------------------------')"],"execution_count":60,"outputs":[{"output_type":"stream","text":["------------------- Get topic sentiment transfer for trump_tweets -----------------------\n","ERROR: Forbidden\n","-------------------------------------------------------------\n"],"name":"stdout"}]},{"metadata":{"id":"tIwLxZESAMPu","colab_type":"text"},"cell_type":"markdown","source":["## Get topic consensus transfer"]},{"metadata":{"id":"O4treKgUAMPz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"ccece605-25d8-47eb-cce7-d5e41ebf6835","executionInfo":{"status":"ok","timestamp":1556512531215,"user_tz":420,"elapsed":287,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset0 = 'trump_tweets'\n","print('------------------- Get topic consensus transfer for {} -----------------------'.format(dataset))\n","\n","dataset1 = None # str | Validation dataset (optional if period_0 and period_1 dates provided)\n","#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n","#fixed_topic is also an available input argument\n","query = ''\n","custom_stop_words = [\"\"] # str | List of stop words. (optional)\n","num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n","num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n","metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n","period_0_start = '2018-08-12' # Not needed if you provide a validation dataset in the \"dataset1\" variable \n","period_0_end = '2018-08-16' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n","period_1_start = '2018-08-14' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n","period_1_end = '2019-08-18' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n","excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n","custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n","\n","try:\n","    payload = nucleus_api.TopicConsensusTransferModel(\n","        dataset0=dataset0,\n","        dataset1=dataset1,\n","        query=query, \n","        custom_stop_words=custom_stop_words, \n","        num_topics=num_topics, \n","        num_keywords=num_keywords,\n","        period_0_start=period_0_start,\n","        period_0_end=period_0_end,\n","        period_1_start=period_1_start,\n","        period_1_end=period_1_end,\n","        metadata_selection=metadata_selection,\n","        custom_dict_file=custom_dict_file)\n","    api_response = api_instance.post_topic_consensus_transfer_api(payload)\n","    api_ok = True\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","    api_ok = False\n","\n","if api_ok:\n","    topics = api_response.result\n","    for i,res in enumerate(topics):\n","        print('Topic', i, 'exposure within validation dataset:')\n","        print('    Keywords:', res.keywords)\n","        print('    Consensus:', res.consensus)\n","        print('---------------')\n","    \n","print('-------------------------------------------------------------')"],"execution_count":61,"outputs":[{"output_type":"stream","text":["------------------- Get topic consensus transfer for trump_tweets -----------------------\n","ERROR: Forbidden\n","-------------------------------------------------------------\n"],"name":"stdout"}]},{"metadata":{"id":"6bS-WcrsAMP6","colab_type":"text"},"cell_type":"markdown","source":["## Get topic delta"]},{"metadata":{"id":"F8hCsHxOAMP9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"47200cf1-004c-4607-9419-7c741b2402d0","executionInfo":{"status":"error","timestamp":1556512544859,"user_tz":420,"elapsed":274,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets'\n","print('------------------- Get topic deltas for {} -----------------------'.format(dataset))\n"," \n","#dataset = dataset # str | Dataset name.\n","#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n","query = ''\n","custom_stop_words = [\"\"] # str | List of stop words. (optional)\n","num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n","num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n","metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n","period_0_start = '2018-08-12'\n","period_0_end = '2018-08-15'\n","period_1_start = '2018-08-16'\n","period_1_end = '2018-08-19'\n","excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n","\n","try:\n","    payload = nucleus_api.TopicDeltaModel(\n","        dataset=dataset, \n","        query=query, \n","        custom_stop_words=custom_stop_words, \n","        num_topics=num_topics, \n","        num_keywords=num_keywords,\n","        period_0_start=period_0_start,\n","        period_0_end=period_0_end,\n","        period_1_start=period_1_start,\n","        period_1_end=period_1_end,\n","        metadata_selection=metadata_selection)\n","    api_response = api_instance.post_topic_delta_api(payload)\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","\n","doc_ids_t0 = api_response.result.doc_ids_t0\n","doc_ids_t1 = api_response.result.doc_ids_t1\n","topics = api_response.result.topics\n","for i,res in enumerate(topics):\n","    print('Topic', i, 'changes in exposure:')\n","    print('    Keywords:', res.keywords)\n","    print('    Document ID:', doc_ids_t0, doc_ids_t1)\n","    print('    Per Source Change in Exposure:', res.doc_topic_exposure_deltas)\n","    print('---------------')\n","    \n","print('-------------------------------------------------------------')"],"execution_count":62,"outputs":[{"output_type":"stream","text":["------------------- Get topic deltas for trump_tweets -----------------------\n","ERROR: Forbidden\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-62-d2d0d80ad7da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mdoc_ids_t0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_ids_t0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mdoc_ids_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_ids_t1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'result'"]}]},{"metadata":{"id":"jrEzNp6QAMQE","colab_type":"text"},"cell_type":"markdown","source":["# Document APIs"]},{"metadata":{"id":"ApkyeUGeAMQI","colab_type":"text"},"cell_type":"markdown","source":["## Get document information without content"]},{"metadata":{"id":"GoxDzdP3AMQJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"43c43d59-3319-426f-fd82-79f8e8015f00","executionInfo":{"status":"error","timestamp":1556512561947,"user_tz":420,"elapsed":372,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets'\n","print('------------------- Get document information for {} -----------------------'.format(dataset))\n","# doc_titles, doc_ids, and metadata_selection below are filters to narrow down \n","# documents to be retrieved.\n","# The information of all documents will be retrived when no filters are provided.\n","\n","# doc_titles: list of strings\n","# The titles of the documents to retrieve. Example: [\"title1\", \"title2\", ..., \"titleN\"]  (optional)\n","# doc_titles = ['D_Trump2018_8_18_1_47']   \n","doc_titles = []\n","# doc_ids: list of strings\n","# The docid of the documents to retrieve. Example: [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n","# doc_ids = ['3397215194896514820', '776902852041351634']\n","doc_ids = []\n","\n","# metadata_selection = {\"author\": \"D_Trump16\"} # dict | A selector off metadata. Example: {\"field\": \"value\"}  (optional)\n","metadata_selection = ''\n","\n","try:\n","    payload = nucleus_api.DocInfo(dataset=dataset, \n","                                doc_titles=doc_titles, \n","                                doc_ids=doc_ids,\n","                                metadata_selection=metadata_selection)\n","    api_response = api_instance.post_doc_info(payload)\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","\n","for res in api_response.result:\n","    print('Document ID:', res.sourceid)\n","    print('    title:', res.title)\n","    for attr in res.attribute.keys():\n","        if attr == 'time':\n","            print('   ', attr, ':', datetime.datetime.fromtimestamp(float(res.attribute[attr])))\n","        else:\n","            print('   ', attr, ':', res.attribute[attr])\n","\n","    print('---------------')\n","\n","print('-------------------------------------------------------------')"],"execution_count":63,"outputs":[{"output_type":"stream","text":["------------------- Get document information for trump_tweets -----------------------\n","ERROR: Forbidden\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-63-e52ca9564d42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Document ID:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msourceid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    title:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'result'"]}]},{"metadata":{"id":"LjlFXtNfAMQP","colab_type":"text"},"cell_type":"markdown","source":["## Display document info with a metadata selection"]},{"metadata":{"id":"WlZXeJFWAMQR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"ec0a2a80-2c6d-42cc-b5e4-52d47aec39f1","executionInfo":{"status":"error","timestamp":1556512571452,"user_tz":420,"elapsed":395,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets' # str | Dataset name.\n","print('------------------- Get document information for {} -----------------------'.format(dataset))\n","\n","metadata_selection = {\"author\": \"D_Trump16\"}      # dict | A selector off metadata. Example: {\"field\": \"value\"}  (optional)\n","\n","try:\n","    payload = nucleus_api.DocInfo(dataset=dataset, metadata_selection=metadata_selection)\n","    api_response = api_instance.post_doc_info(payload)\n","    \n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","\n","for res in api_response.result:\n","    print('Document ID:', res.sourceid)\n","    print('    title:', res.title)\n","    for attr in res.attribute.keys():\n","        if attr == 'time':\n","            print('   ', attr, ':', datetime.datetime.fromtimestamp(float(res.attribute[attr])))\n","        else:\n","            print('   ', attr, ':', res.attribute[attr])\n","\n","    print('---------------')\n","\n","print('-------------------------------------------------------------')"],"execution_count":64,"outputs":[{"output_type":"stream","text":["------------------- Get document information for trump_tweets -----------------------\n","ERROR: Forbidden\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-64-c85ff7b9d837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Document ID:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msourceid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    title:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'result'"]}]},{"metadata":{"id":"4zFmtynpAMQW","colab_type":"text"},"cell_type":"markdown","source":["## Display document details"]},{"metadata":{"scrolled":true,"id":"yJOjTHPIAMQY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"d593c6a3-a3ab-4509-a2cc-066ab042c4b8","executionInfo":{"status":"error","timestamp":1556512576714,"user_tz":420,"elapsed":291,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets' # str | Dataset name.\n","print('------------------- Get document details for {} -----------------------'.format(dataset))\n","\n","#doc_titles = ['D_Trump2018_8_18_1_47']   # str | The title of the documents to retrieve. Example: [\"title1\", \"title2\", ..., \"titleN\"]  (optional)\n","doc_ids = ['776902852041351634']      # str | The docid of the documents to retrieve. Example: [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n","\n","try:\n","    payload = nucleus_api.DocDisplay(dataset, doc_ids=doc_ids)\n","    api_response = api_instance.post_doc_display(payload)\n","    \n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","\n","for res in api_response.result:\n","    print('Document ID:', res.sourceid)\n","    print('    Title:', res.title)\n","    print('    Author:', res.attribute['author'])\n","    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute['time'])))\n","    print('    Content', res.content)\n","\n","    print('---------------')\n","\n","print('-------------------------------------------------------------')"],"execution_count":65,"outputs":[{"output_type":"stream","text":["------------------- Get document details for trump_tweets -----------------------\n","ERROR: Forbidden\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-65-6d378432a7bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Document ID:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msourceid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    Title:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'result'"]}]},{"metadata":{"id":"yNIvOFupAMQj","colab_type":"text"},"cell_type":"markdown","source":["## Display document details with a metadata selection"]},{"metadata":{"id":"Ds1hw9oqAMQy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"c8dbc87c-cedc-4a23-e8ff-30c799c90580","executionInfo":{"status":"error","timestamp":1556512583691,"user_tz":420,"elapsed":488,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets' # str | Dataset name.\n","print('------------------- Get document details for {} -----------------------'.format(dataset))\n","metadata_selection = {\"author\": \"D_Trump16\"}      # dict | A selector off metadata. Example: {\"field\": \"value\"}  (optional)\n","\n","try:\n","    payload = nucleus_api.DocDisplay(dataset=dataset, metadata_selection=metadata_selection)\n","    api_response = api_instance.post_doc_display(payload)\n","    \n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","\n","for res in api_response.result:\n","    print('Document ID:', res.sourceid)\n","    print('    Title:', res.title)\n","    print('    Author:', res.attribute['author'])\n","    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute['time'])))\n","    print('    Content', res.content)\n","\n","    print('---------------')\n","\n","print('-------------------------------------------------------------')"],"execution_count":66,"outputs":[{"output_type":"stream","text":["------------------- Get document details for trump_tweets -----------------------\n","ERROR: Forbidden\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-66-68b98a4fba95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Document ID:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msourceid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    Title:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'result'"]}]},{"metadata":{"id":"AWcRhjFEAMRP","colab_type":"text"},"cell_type":"markdown","source":["## Get document recommendations"]},{"metadata":{"id":"TzMCCD3MAMRT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"88f03f87-4a3e-4f0a-9169-2c53b2010be0","executionInfo":{"status":"error","timestamp":1556512591795,"user_tz":420,"elapsed":338,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets' # str | Dataset name.\n","print('------------- Get document recommendations for {} -----------------'.format(dataset))\n","\n","#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n","query = ''\n","custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n","num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n","num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n","excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n","\n","try:\n","    payload = nucleus_api.DocumentRecommendModel(dataset=dataset, \n","                                                query=query, \n","                                                custom_stop_words=custom_stop_words, \n","                                                num_topics=num_topics, \n","                                                num_keywords=num_keywords)\n","    api_response = api_instance.post_doc_recommend_api(payload)\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","    \n","for i, res in enumerate(api_response.result):\n","    print('Document recommendations for topic', i, ':')\n","    print('    Keywords:', res.keywords)\n","\n","    for j, doc in enumerate(res.recommendations):\n","        print('    Recommendation', j, ':')\n","        print('        Document ID:', doc.sourceid)\n","        print('        Title:', doc.title)\n","        print('        Attribute:', doc.attribute)\n","        print('        Author:', doc.attribute['author'])\n","        print('        Time:', datetime.datetime.fromtimestamp(float(doc.attribute['time'])))\n","    \n","    print('---------------')\n","    \n","print('-------------------------------------------------------------')"],"execution_count":67,"outputs":[{"output_type":"stream","text":["------------- Get document recommendations for trump_tweets -----------------\n","ERROR: Forbidden\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-b54bda98bd9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Document recommendations for topic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    Keywords:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'result'"]}]},{"metadata":{"id":"d3d0UVk8AMRZ","colab_type":"text"},"cell_type":"markdown","source":["## Get document summary"]},{"metadata":{"scrolled":true,"id":"FxurDldGAMRb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"33e7ea02-fd64-4393-daaf-ee4fece7e753","executionInfo":{"status":"ok","timestamp":1556512599401,"user_tz":420,"elapsed":278,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets' # str | Dataset name.\n","doc_title = 'D_Trump2018_8_17_14_10' # str | The title of the document to be summarized.\n","print('------------------ Get document summary for {} in {}  --------------------'.format(doc_title, dataset))\n","\n","custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n","summary_length = 6 # int | The maximum number of bullet points a user wants to see in the document summary. (optional) (default to 6)\n","context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n","short_sentence_length = 0 # int | The sentence length below which a sentence is excluded from summarization (optional) (default to 4)\n","long_sentence_length = 40 # int | The sentence length beyond which a sentence is excluded from summarization (optional) (default to 40)\n","\n","try:\n","    payload = nucleus_api.DocumentSummaryModel(dataset=dataset, \n","                                            doc_title=doc_title, \n","                                            custom_stop_words=custom_stop_words, \n","                                            summary_length=summary_length, \n","                                            context_amount=context_amount,\n","                                            short_sentence_length=short_sentence_length,\n","                                            long_sentence_length=long_sentence_length)\n","    api_response = api_instance.post_doc_summary_api(payload)\n","    \n","    print('Summary for', api_response.result.doc_title)\n","    for sent in api_response.result.summary.sentences:\n","        print('    *', sent)\n","    \n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","\n","print('-------------------------------------------------------------')"],"execution_count":68,"outputs":[{"output_type":"stream","text":["------------------ Get document summary for D_Trump2018_8_17_14_10 in trump_tweets  --------------------\n","ERROR: Forbidden\n","-------------------------------------------------------------\n"],"name":"stdout"}]},{"metadata":{"id":"ICZ9if3hAMRi","colab_type":"text"},"cell_type":"markdown","source":["## Get document sentiment"]},{"metadata":{"id":"CBg2EbmwAMRj","colab_type":"code","colab":{}},"cell_type":"code","source":["dataset = 'trump_tweets' # str | Dataset name.\n","doc_title = 'D_Trump2018_8_17_14_10' # str | The title of the document to be analyzed.\n","print('------------------ Get document sentiment  for {} in {}  --------------------'.format(doc_title, dataset))\n","\n","custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n","num_topics = 8 # int | Number of topics to be extracted from the document. (optional) (default to 8)\n","num_keywords = 8 # int | Number of keywords per topic that is extracted from the document. (optional) (default to 8)\n","\n","try:\n","    payload = nucleus_api.DocumentSentimentModel(dataset=dataset, \n","                                                doc_title=doc_title, \n","                                                custom_stop_words=custom_stop_words, \n","                                                num_topics=num_topics, \n","                                                num_keywords=num_keywords)\n","    api_response = api_instance.post_doc_sentiment_api(payload)\n","    \n","    print('Sentiment for', api_response.result.doc_title)\n","    print(api_response.result.sentiment)\n","\n","except ValueError as e:\n","    print('ERROR:', e)\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])\n","\n","print('-------------------------------------------------------------')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4bj2THrOAMRs","colab_type":"text"},"cell_type":"markdown","source":["## Tag documents"]},{"metadata":{"id":"qfC8Wpa3AMRu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"30e97ada-5023-4c2d-d16a-2264d0406fda","executionInfo":{"status":"ok","timestamp":1556512617875,"user_tz":420,"elapsed":401,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["dataset = 'trump_tweets' # str | Dataset name.\n","print('---------------- Tag dataset ------------------------')\n","\n","try:\n","    payload = nucleus_api.DatasetTagging(dataset=dataset, \n","                                        query='new york city OR big apple OR NYC OR New York', \n","                                        metadata_selection='', \n","                                        time_period='',\n","                                        period_start='2010-01-01',\n","                                        period_end='2019-04-30')\n","    api_response = api_instance.post_dataset_tagging(payload)\n","    print(api_response)\n","    print('    Entities tagged:', api_response.result.entities_tagged)\n","    print('    Docids tagged with the entities:', api_response.result.doc_ids)\n","    print('    Entities count:', api_response.result.entities_count)\n","except ApiException as e:\n","    api_error = json.loads(e.body)\n","    print('ERROR:', api_error['message'])"],"execution_count":69,"outputs":[{"output_type":"stream","text":["---------------- Tag dataset ------------------------\n","ERROR: Forbidden\n"],"name":"stdout"}]},{"metadata":{"id":"VTiz97-0AMSR","colab_type":"text"},"cell_type":"markdown","source":["## Summarize file from URL "]},{"metadata":{"id":"qvSvyX93AMST","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":489},"outputId":"f4a07449-dc11-4347-fc86-50f73dabbf97","executionInfo":{"status":"error","timestamp":1556512623072,"user_tz":420,"elapsed":372,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["######################################################################################\n","# file_params fields descriptions:  \n","#   file_url              : string, the URL at which the file is stored (could be a S3 bucket address for instance)\n","#   filename              : OPTIONAL string, filename saved on the server. also serves as the doc_title for summarization\n","#   custom_stop_words     : OPTIONAL a string list, user-provided list of stopwords to be excluded from the content analysis leading to document summarization\n","#                            [\"word1\", \"word2\", ...]. DEFAULT: empty\n","#   summary_length        : OPTIONAL an integer, the maximum number of bullet points a user wants to see in the document summary. DEFAULT: 6\n","#   context_amount        : OPTIONAL an integer, the number of sentences surrounding key summary sentences in the original document that a user wants to see in the document summary. DEFAULT: 0\n","#   short_sentence_length : OPTIONAL an integer, the sentence length below which a sentence is excluded from summarization. DEFAULT: 4 words\n","#   long_sentence_length  : OPTIONAL an integer, the sentence length beyond which a sentence is excluded from summarization. DEFAULT: 40 words\n","#\n","file_params = {\n","    'file_url': 'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx',\n","    'filename': 'quarles20181109a-newname.pdf',   \n","    'custom_stop_words': [\"document\", \"sometimes\"], \n","    'summary_length': 6,\n","    'context_amount': 0, \n","    'short_sentence_length': 4, \n","    'long_sentence_length': 40}\n","\n","result = nucleus_helper.summarize_file_url(api_instance, file_params)\n","  \n","print('Summary for', result.doc_title, ':')\n","for sent in result.summary.sentences:\n","    print('    *', sent)\n","\n","print('-------------------------------------------------------------')"],"execution_count":70,"outputs":[{"output_type":"stream","text":["Exception when calling DatasetsApi->post_upload_url: (403)\n","Reason: Forbidden\n","HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 29 Apr 2019 04:38:30 GMT', 'Content-Type': 'application/json', 'Content-Length': '24', 'Connection': 'keep-alive', 'x-amzn-RequestId': 'a3a8c442-6a38-11e9-bef5-c3daf08f0f36', 'x-amzn-ErrorType': 'ForbiddenException', 'x-amz-apigw-id': 'Y4jnDEASPHcFTTQ='})\n","HTTP response body: {\"message\":\"Forbidden\"}\n","\n","\n","\n","Exception when calling DocumentsApi->get_doc_summary_api: (403)\n","Reason: Forbidden\n","HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 29 Apr 2019 04:38:30 GMT', 'Content-Type': 'application/json', 'Content-Length': '24', 'Connection': 'keep-alive', 'x-amzn-RequestId': 'a3b19e58-6a38-11e9-9e18-ef3d46753a49', 'x-amzn-ErrorType': 'ForbiddenException', 'x-amz-apigw-id': 'Y4jnEGyDPHcFyXg='})\n","HTTP response body: {\"message\":\"Forbidden\"}\n","\n","\n","\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-70-f1d67969cdd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnucleus_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize_file_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Summary for'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    *'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'doc_title'"]}]},{"metadata":{"id":"x4G1eVsCAMSa","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}